{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoLeftEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"console\"]}\n",
    "\n",
    "    LEFT = 0\n",
    "    RIGHT = 1\n",
    "\n",
    "    def __init__(self, grid_size=10, render_mode=\"console\"):\n",
    "        super(GoLeftEnv, self).__init__()\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.agent_pos = grid_size - 1\n",
    "\n",
    "        n_actions = 2\n",
    "        self.action_space = spaces.Discrete(n_actions)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=self.grid_size,\n",
    "            shape=(1,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed, options=options)\n",
    "\n",
    "        self.agent_pos = self.grid_size - 1\n",
    "\n",
    "        return np.array([self.agent_pos]).astype(np.float32), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.LEFT:\n",
    "            self.agent_pos -= 1\n",
    "        elif action == self.RIGHT:\n",
    "            self.agent_pos += 1\n",
    "        else:\n",
    "            raise ValueError(f\"Received inavlid action={action}\")\n",
    "\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "        terminated = bool(self.agent_pos == 0)\n",
    "        truncated = False\n",
    "\n",
    "        reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return (\n",
    "            np.array([self.agent_pos]).astype(np.float32),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"console\":\n",
    "            print(\".\" * self.agent_pos, end=\"\")\n",
    "            print(\"X\", end=\"\")\n",
    "            print(\".\" * (self.grid_size - self.agent_pos - 1))\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GoLeftEnv()\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........X\n",
      "Box(0.0, 10.0, (1,), float32)\n",
      "Discrete(2)\n",
      "1\n",
      "Step 1\n",
      "obs= [8.] reward= 0 done= False\n",
      "........X.\n",
      "Step 2\n",
      "obs= [7.] reward= 0 done= False\n",
      ".......X..\n",
      "Step 3\n",
      "obs= [6.] reward= 0 done= False\n",
      "......X...\n",
      "Step 4\n",
      "obs= [5.] reward= 0 done= False\n",
      ".....X....\n",
      "Step 5\n",
      "obs= [4.] reward= 0 done= False\n",
      "....X.....\n",
      "Step 6\n",
      "obs= [3.] reward= 0 done= False\n",
      "...X......\n",
      "Step 7\n",
      "obs= [2.] reward= 0 done= False\n",
      "..X.......\n",
      "Step 8\n",
      "obs= [1.] reward= 0 done= False\n",
      ".X........\n",
      "Step 9\n",
      "obs= [0.] reward= 1 done= True\n",
      "X.........\n",
      "Goal reached! reward= 1\n"
     ]
    }
   ],
   "source": [
    "env = GoLeftEnv(grid_size=10)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.action_space.sample())\n",
    "\n",
    "GO_LEFT = 0\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "    print(f\"Step {step + 1}\")\n",
    "    obs, reward, terminated, truncated, info = env.step(GO_LEFT)\n",
    "    done = terminated or truncated\n",
    "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"Goal reached!\", \"reward=\", reward)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = int(np.random.randint(0, np.iinfo(np.uint32).max, dtype=np.uint32))\n",
    "vec_env = make_vec_env(GoLeftEnv, n_envs=1, env_kwargs=dict(grid_size=10), seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 19.2     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 363      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.103   |\n",
      "|    explained_variance | -0.59    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.00364 |\n",
      "|    value_loss         | 0.0271   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 12.6      |\n",
      "|    ep_rew_mean        | 1         |\n",
      "| time/                 |           |\n",
      "|    fps                | 452       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0489   |\n",
      "|    explained_variance | -0.0195   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -1.11e-05 |\n",
      "|    value_loss         | 0.000213  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 9.4       |\n",
      "|    ep_rew_mean        | 1         |\n",
      "| time/                 |           |\n",
      "|    fps                | 487       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.062    |\n",
      "|    explained_variance | -3.41     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -0.000501 |\n",
      "|    value_loss         | 0.00175   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 9.3       |\n",
      "|    ep_rew_mean        | 1         |\n",
      "| time/                 |           |\n",
      "|    fps                | 508       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.032    |\n",
      "|    explained_variance | -0.0152   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -0.000252 |\n",
      "|    value_loss         | 0.00197   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.12     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 520      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0393  |\n",
      "|    explained_variance | -0.747   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.000474 |\n",
      "|    value_loss         | 0.00489  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 9.08      |\n",
      "|    ep_rew_mean        | 1         |\n",
      "| time/                 |           |\n",
      "|    fps                | 528       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0205   |\n",
      "|    explained_variance | -4.01     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0.000102 |\n",
      "|    value_loss         | 0.00217   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.06     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 537      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0123  |\n",
      "|    explained_variance | 0.782    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 8.75e-05 |\n",
      "|    value_loss         | 0.000745 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.02     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 543      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0129  |\n",
      "|    explained_variance | 0.499    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 6.93e-05 |\n",
      "|    value_loss         | 0.00175  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 9         |\n",
      "|    ep_rew_mean        | 1         |\n",
      "| time/                 |           |\n",
      "|    fps                | 549       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00388  |\n",
      "|    explained_variance | 0.953     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -5.25e-06 |\n",
      "|    value_loss         | 0.000152  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 9         |\n",
      "|    ep_rew_mean        | 1         |\n",
      "| time/                 |           |\n",
      "|    fps                | 553       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0105   |\n",
      "|    explained_variance | 0.492     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -3.85e-05 |\n",
      "|    value_loss         | 0.000713  |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = A2C(\"MlpPolicy\", env,  verbose=1).learn(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "Action:  [0]\n",
      "obs= [[8.]] reward= [0.] done= [False]\n",
      "........X.\n",
      "Step 2\n",
      "Action:  [0]\n",
      "obs= [[7.]] reward= [0.] done= [False]\n",
      ".......X..\n",
      "Step 3\n",
      "Action:  [0]\n",
      "obs= [[6.]] reward= [0.] done= [False]\n",
      "......X...\n",
      "Step 4\n",
      "Action:  [0]\n",
      "obs= [[5.]] reward= [0.] done= [False]\n",
      ".....X....\n",
      "Step 5\n",
      "Action:  [0]\n",
      "obs= [[4.]] reward= [0.] done= [False]\n",
      "....X.....\n",
      "Step 6\n",
      "Action:  [0]\n",
      "obs= [[3.]] reward= [0.] done= [False]\n",
      "...X......\n",
      "Step 7\n",
      "Action:  [0]\n",
      "obs= [[2.]] reward= [0.] done= [False]\n",
      "..X.......\n",
      "Step 8\n",
      "Action:  [0]\n",
      "obs= [[1.]] reward= [0.] done= [False]\n",
      ".X........\n",
      "Step 9\n",
      "Action:  [0]\n",
      "obs= [[9.]] reward= [1.] done= [ True]\n",
      ".........X\n",
      "Goal reached! reward= [1.]\n"
     ]
    }
   ],
   "source": [
    "obs = vec_env.reset()\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    print(f\"Step {step + 1}\")\n",
    "    print(\"Action: \", action)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        print(\"Goal reached!\", \"reward=\", reward)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinfocus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
