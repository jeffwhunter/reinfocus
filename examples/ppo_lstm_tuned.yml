# Example of potentially tuned hyperparameters for ppo_lstm
examples.custom_environments.DiscreteSteps-v0:
  normalize: true
  n_envs: 8
  n_timesteps: !!float 5e5
  policy: 'MlpLstmPolicy'
  batch_size: 32
  n_steps: 32
  gamma: 0.95
  learning_rate: 0.0001
  ent_coef: 0.001
  clip_range: 0.3
  n_epochs: 5
  gae_lambda: 0.95
  max_grad_norm: 0.5
  vf_coef: 0.25
  use_sde: true
  sde_sample_freq: 16
  policy_kwargs: "dict(
                    log_std_init=-2.0,
                    ortho_init=False,
                    activation_fn=nn.ReLU,
                    lstm_hidden_size=16,
                    enable_critic_lstm=True,
                    net_arch=dict(pi=[64], vf=[64])
                  )"
